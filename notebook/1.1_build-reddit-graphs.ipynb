{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we label users as supporters of Clinton or Trump; we build the graph of comments in `/r/politics` for the labelled users; we analyze these graphs according to the labels, and considering the sentiment.\n",
    "\n",
    "The notebook is divided into two parts: one where we compute the graphs, the other of analysis.\n",
    "The second part can be executed without the first one, if the necessary processed data files are available.\n",
    "\n",
    "The main outputs are:\n",
    "\n",
    "*First part:*\n",
    "1. A file with labelled users (merged with their geolocalization from Balsamo et al, WebConf 2019).\n",
    "2. A file for each graph\n",
    "\n",
    "*Second part:*\n",
    "3. The interaction matrix, with the number of edges for each combination of labels for each graph.\n",
    "4. The average sentiment for each combination of labels for `r/politics`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and output paths\n",
    "\n",
    "You can download these files from [https://files.pushshift.io/reddit/](https://files.pushshift.io/reddit/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_path = '../data/' #'/data/big/reddit/submissions/2016/RS_2016-*.bz2'\n",
    "comments_path = '../data/' #'/data/big/reddit/comments/2016/RC_2016-*.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the home communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDIT_HOME_TRUMP = {'The_Donald'}\n",
    "SUBREDDIT_HOME_CLINTON = {'hillaryclinton', 'HillaryForAmerica'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Trump and Clinton users using posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️\n",
    "\n",
    "**This part can be skipped.**\n",
    "The output can be recovered just by reloading the files.\n",
    "\n",
    "⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra imports, just for this part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there is one file per month\n",
    "assert len(glob(comments_path)) == 12\n",
    "assert len(glob(posts_path)) == 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain all the users that posted in any of the home communities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_rdd = sc.textFile(posts_path).map(json.loads)\n",
    "trump_posts_rdd   = posts_rdd.filter(lambda x: 'subreddit' in x and x['subreddit'] in SUBREDDIT_HOME_TRUMP)\n",
    "clinton_posts_rdd = posts_rdd.filter(lambda x: 'subreddit' in x and x['subreddit'] in SUBREDDIT_HOME_CLINTON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_ncom_avgscore, clinton_ncom_avgscore = [\n",
    "    rdd\n",
    "     .filter(lambda x: x['author'] not in {'[deleted]', })\n",
    "     .map(lambda x: (x['author'], x['score']))\n",
    "     .groupByKey() # Result: author -> [post_score_0, ..., post_score_N]\n",
    "     .map(lambda x: (x[0], len(x[1]), sum(x[1]) / len(x[1])))\n",
    "     # Result: author, number of posts, average score\n",
    "                                              \n",
    "    for rdd in (trump_posts_rdd, clinton_posts_rdd)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to disambiguate the intersection and to remove trolls: let's look at the reddit scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2trumpscore = dict(trump_ncom_avgscore  .filter(lambda x: x[2] >= 1).map(lambda x: (x[0], x[1])).collect())\n",
    "u2clintscore = dict(clinton_ncom_avgscore.filter(lambda x: x[2] >= 1).map(lambda x: (x[0], x[1])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = set(u2clintscore.keys()) & set(u2trumpscore.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the users in the intersection, how are scores distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_hc_scores = np.array([(u2trumpscore[u], u2clintscore[u]) for u in list(intersection)])\n",
    "\n",
    "par = plt.hist2d(*dt_hc_scores.T, bins=np.arange(1, 10, 1), norm=matplotlib.colors.LogNorm())\n",
    "\n",
    "plt.xlabel(\"Trump score\")\n",
    "plt.ylabel(\"Clinton score\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few have a high score in _both_, so let's define the labels in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = {u for u, score in u2trumpscore.items() if (u not in u2clintscore or u2clintscore.get(u) < score)}\n",
    "dem = {u for u, score in u2clintscore.items() if (u not in u2trumpscore or u2trumpscore.get(u) < score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rep), len(dem), len(rep & dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save these labels, together with Duilio's geolocalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.DataFrame([(u, 'R') for u in rep] + [(u, 'D') for u in dem], columns=['author', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_location_original = pd.read_csv(\"../data/raw/author_locations_16_17_new_opiates.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_label_state = pd.merge(authors, author_location_original[['author', 'state']], how='left', on='author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_label_state.to_csv(\"../data/processed/author-label-state.csv.bz2\", compression='bz2', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_set = set(author_label_state.author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the graph of comments in `/r/politics` for the labelled users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also analyze and save the sentiment of each comment (so, the sentiment expressed by the child answering to the parent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comments_path)\n",
    "\n",
    "comments_rdd = sc.textFile(comments_path).map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comment_graph_for_subreddit(comments_rdd, authors_set, subreddits_list, graph_output_path):\n",
    "    \"\"\"\n",
    "    Take all the comments in the given RDD, and select only those\n",
    "    in the given list of subreddits AND between authors included in authors_set.\n",
    "    Saves this as a graph with parent_author, child_author, child_sentiment. \n",
    "    \n",
    "    TAKES A VERY LONG TIME!\n",
    "    \"\"\"\n",
    "    \n",
    "    rdd_selected_comments = comments_rdd.filter(\n",
    "        lambda x: \n",
    "                'author' in x.keys() and\n",
    "                x['author'] in authors_set and\n",
    "                'subreddit' in x.keys() and\n",
    "                x['subreddit'] in subreddits_list\n",
    "        )\n",
    "\n",
    "    rdd_selected_parent_author = (rdd_selected_comments\n",
    "        .filter(lambda x: 'parent_id' in x)\n",
    "        .map(lambda x: (x['parent_id'].replace('t1_',''), (x['author'], x['body'])))\n",
    "    )\n",
    "\n",
    "    # Result: parent_id -> (author, body)\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    edges_rdd = (rdd_selected_comments\n",
    "        .map(lambda x: (x['id'], x['author']))\n",
    "        .join(rdd_selected_parent_author) # Result: parent_id -> [ parent_author, (child_author, child_body) ]\n",
    "        .map(lambda x: (x[1][0], x[1][1][0], analyzer.polarity_scores(x[1][1][1])['compound']))\n",
    "    )\n",
    "\n",
    "    # Result: parent_author, child_author, child_sentiment\n",
    "\n",
    "    edges_rdd.map(lambda x: ','.join(str(d) for d in x)).repartition(1).saveAsTextFile(\n",
    "        graph_output_path + \"-folder\",\n",
    "        compressionCodecClass='org.apache.hadoop.io.compress.BZip2Codec')\n",
    "    df = pd.read_csv(graph_output_path + \"-folder/part-00000.bz2\",\n",
    "           names=['parent', 'child', 'sentiment'])\n",
    "    df.to_csv(graph_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_comment_graph_for_subreddit(comments_rdd, authors_set, {'politics'}, \n",
    "                                   OUTPUT_PATH + 'parent_child_sentiment_edges_politics.csv.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved the graph, as triplets parent, child, sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️\n",
    "\n",
    "**From here, it can be executed without the part before** just reloading the files:\n",
    "\n",
    "⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️ ⚠️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count interactions in the `r/politics` graph by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_csv(OUTPUT_PATH + \"parent_child_sentiment_edges_politics.csv.bz2\")\n",
    "\n",
    "author_label_state = pd.read_csv(\"../data/processed/author-label-state.csv.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label each edge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "author2label = dict(author_label_state[['author', 'label']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_labels_to_graph_dataframe(edges, author2label):\n",
    "    edges['lparent'] = edges.parent.map(author2label.get)\n",
    "    edges['lchild'] = edges.child.map(author2label.get)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = attach_labels_to_graph_dataframe(edges, author2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many edges do we have for each label?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lparent\n",
       "D    246838\n",
       "R    469927\n",
       "Name: parent, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.groupby('lparent').parent.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lchild\n",
       "D    247052\n",
       "R    469713\n",
       "Name: parent, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.groupby('lchild').parent.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The interaction matrix for `/r/politics`!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lparent  lchild\n",
       "D        D          69800\n",
       "         R         177038\n",
       "R        D         177252\n",
       "         R         292675\n",
       "Name: child, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.groupby(['lparent', 'lchild']).child.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As join probabilities: P(R, R), P(R, D), P(D, R), P(D, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lparent  lchild\n",
       "D        D         0.097382\n",
       "         R         0.246996\n",
       "R        D         0.247294\n",
       "         R         0.408328\n",
       "Name: child, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.groupby(['lparent', 'lchild']).child.count() / len(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look also at the average sentiment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lparent  lchild\n",
       "D        D         0.057470\n",
       "         R         0.007168\n",
       "R        D         0.011035\n",
       "         R         0.012603\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.groupby(['lparent', 'lchild']).sentiment.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emotional contagion! RR is one extreme, DD is the other, and they move towards each other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
